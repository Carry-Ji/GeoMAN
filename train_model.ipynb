{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tensorflow04/anaconda3/envs/xuchao/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/tensorflow04/anaconda3/envs/xuchao/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "from utils import load_data\n",
    "from utils import load_global_inputs\n",
    "from utils import basic_hyperparams\n",
    "#from GeoMAN import GeoMAN\n",
    "from utils import shuffle_data\n",
    "from utils import get_batch_feed_dict\n",
    "from utils import get_valid_batch_feed_dict\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_hidden_encoder': 64, 'n_output_decoder': 1, 'n_steps_encoder': 5, 'learning_rate': 0.001, 'n_sensors': 307, 'n_external_input': 83, 'gc_rate': 2.5, 'lambda_l2_reg': 0.001, 'dropout_rate': 0.3, 'n_hidden_decoder': 64, 'n_steps_decoder': 5, 'n_input_decoder': 1, 'n_stacked_layers': 2, 'n_input_encoder': 3, 's_attn_flag': 2, 'ext_flag': False}\n"
     ]
    }
   ],
   "source": [
    "# load hyperparameters\n",
    "hps = basic_hyperparams()\n",
    "print(hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './sample_data/GeoMAN-5-5-train-local_inputs.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8a487f3890e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./sample_data/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m training_data = load_data(\n\u001b[0;32m----> 4\u001b[0;31m     input_path, 'train', hps['n_steps_encoder'], hps['n_steps_decoder'])\n\u001b[0m\u001b[1;32m      5\u001b[0m valid_data = load_data(\n\u001b[1;32m      6\u001b[0m     input_path, 'eval', hps['n_steps_encoder'], hps['n_steps_decoder'])\n",
      "\u001b[0;32m/home/xuchao/th_geoman/utils.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(input_path, mode, n_steps_encoder, n_steps_decoder)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \"\"\"\n\u001b[1;32m     77\u001b[0m     mode_local_inp = np.load(\n\u001b[0;32m---> 78\u001b[0;31m         input_path + \"GeoMAN-{}-{}-{}-local_inputs.npy\".format(n_steps_encoder, n_steps_decoder, mode))\n\u001b[0m\u001b[1;32m     79\u001b[0m     global_attn_index = np.load(\n\u001b[1;32m     80\u001b[0m         input_path + \"GeoMAN-{}-{}-{}-global_attn_state_indics.npy\".format(n_steps_encoder, n_steps_decoder, mode))\n",
      "\u001b[0;32m~/anaconda3/envs/xuchao/lib/python3.5/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './sample_data/GeoMAN-5-5-train-local_inputs.npy'"
     ]
    }
   ],
   "source": [
    "# read data from different sets\n",
    "input_path = './sample_data/'\n",
    "training_data = load_data(\n",
    "    input_path, 'train', hps['n_steps_encoder'], hps['n_steps_decoder'])\n",
    "valid_data = load_data(\n",
    "    input_path, 'eval', hps['n_steps_encoder'], hps['n_steps_decoder'])\n",
    "global_inpts, global_attn_sts = load_global_inputs(\n",
    "    input_path, hps['n_steps_encoder'], hps['n_steps_decoder'])\n",
    "# print dataset info\n",
    "num_train = len(training_data[0])\n",
    "num_valid = len(valid_data[0])\n",
    "print('train samples: {0}'.format(num_train))\n",
    "print('eval samples: {0}'.format(num_valid))\n",
    "#[mode_local_inp, global_inp_index, global_attn_index, mode_ext_inp, mode_labels]\n",
    "print(training_data[0].shape)\n",
    "print(training_data[1].shape)\n",
    "print(training_data[2].shape)\n",
    "print(training_data[3].shape)\n",
    "print(training_data[4].shape)\n",
    "#global_inputs, global_attn_states\n",
    "print(global_inpts.shape)\n",
    "print(global_attn_sts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2017)\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as tf\n",
    "from utils import Linear\n",
    "from torch.nn import init\n",
    "\n",
    "def input_transform(x):\n",
    "    local_inputs,global_inputs,external_inputs,local_attn_states,global_attn_states,labels = x\n",
    "    batch_size = labels.data.size(0)\n",
    "    n_steps_decoder = labels.data.size(1)\n",
    "    n_output_decoder = labels.data.size(2)\n",
    "    #print(global_inputs.data.size())\n",
    "    n_sensors = global_inputs.data.size(2)\n",
    "    n_steps_encoder = local_inputs.data.size(1)\n",
    "    n_input_encoder = local_inputs.data.size(2)\n",
    "    n_external_input = external_inputs.data.size(2)\n",
    "\n",
    "    # a tuple composed of the local and global attention states\n",
    "    #print(local_attn_states.size(), global_attn_states.size())\n",
    "    encoder_attention_states = (local_attn_states, global_attn_states)\n",
    "\n",
    "    # transform the inputs from local and global view into encoder_inputs\n",
    "    _local_inputs = local_inputs.permute(1, 0, 2)\n",
    "    _local_inputs = _local_inputs.contiguous().view(-1, n_input_encoder)\n",
    "    _local_inputs = torch.split(_local_inputs, batch_size, 0)\n",
    "    _global_inputs = global_inputs.permute(1, 0, 2)\n",
    "    _global_inputs = _global_inputs.contiguous().view(-1, n_sensors)\n",
    "    _global_inputs = torch.split(_global_inputs, batch_size, 0)\n",
    "    encoder_inputs = (_local_inputs, _global_inputs)\n",
    "\n",
    "    # transform the variables into lists as the input of different function\n",
    "    _labels = labels.permute(1, 0, 2)\n",
    "    _labels = _labels.contiguous().view(-1, n_output_decoder)\n",
    "    _labels = torch.split(_labels, batch_size, 0)\n",
    "    #print(_labels[0].size())\n",
    "    _external_inputs = external_inputs.permute(1, 0, 2)\n",
    "    _external_inputs = _external_inputs.contiguous().view(-1, n_external_input)\n",
    "    _external_inputs = torch.split(_external_inputs, batch_size, 0)\n",
    "    #print(_external_inputs[0].size())\n",
    "    # not useful when the loop function is employed\n",
    "    decoder_inputs = [torch.zeros_like(_labels[0])] + list(_labels[:-1])\n",
    "    \n",
    "    return encoder_attention_states, encoder_inputs, _labels, _external_inputs, decoder_inputs\n",
    "\n",
    "class GeoMAN(nn.Module):\n",
    "    def __init__(self, hps):\n",
    "        super(GeoMAN, self).__init__()\n",
    "        self.hps = hps\n",
    "        self.w_out = nn.Parameter(torch.FloatTensor(hps['n_hidden_decoder'], \n",
    "                                                    hps['n_output_decoder']))\n",
    "        self.b_out = nn.Parameter(torch.FloatTensor(hps['n_output_decoder']))\n",
    "        self.encoder_cell = nn.LSTMCell(hps['n_input_encoder']+hps['n_sensors'], hps['n_hidden_encoder'], bias=True)\n",
    "        self.decoder_cell = nn.LSTMCell(hps['n_input_decoder'], hps['n_hidden_decoder'], bias=True)\n",
    "        \n",
    "        init.xavier_uniform(self.w_out)\n",
    "        init.normal(self.b_out)                          \n",
    "\n",
    "    def spatial_attention(self, encoder_inputs, attention_states, cell, s_attn_flag=2, output_size=64):\n",
    "        \n",
    "        local_inputs = encoder_inputs[0]\n",
    "        #print(local_inputs[0].size())\n",
    "        global_inputs = encoder_inputs[1]\n",
    "        local_attention_states = attention_states[0]\n",
    "        global_attention_states = attention_states[1]\n",
    "        #local_inputs is a tuple\n",
    "        batch_size = local_inputs[0].size(0)\n",
    "        #print(batch_size)\n",
    "        \n",
    "        # decide whether to use local/global attention\n",
    "        # s_attn_flag: 0: only local. 1: only global. 2: local + global\n",
    "        local_flag = True\n",
    "        global_flag = True\n",
    "        if s_attn_flag == 0:\n",
    "            global_flag = False\n",
    "        elif s_attn_flag == 1:\n",
    "            local_flag = False\n",
    "        \n",
    "        if local_flag:\n",
    "            local_attn_length = local_attention_states.data.size(1) # n_input_encoder\n",
    "            local_attn_size = local_attention_states.data.size(2) # n_steps_encoder\n",
    "            # A trick: to calculate U_l * x^{i,k} by a 1-by-1 convolution\n",
    "            local_hidden = local_attention_states.contiguous().view(-1, local_attn_size, local_attn_length, 1)\n",
    "            # Size of query vectors for attention.\n",
    "            local_attention_vec_size = local_attn_size\n",
    "            local_u = nn.Conv2d(local_attn_size, local_attention_vec_size, (1,1), (1, 1))\n",
    "            #print(local_hidden.size())\n",
    "            local_hidden_features = local_u(local_hidden.float())\n",
    "            \n",
    "            local_v = nn.Parameter(torch.FloatTensor(local_attention_vec_size))                   \n",
    "            #local_v = Variable(torch.zeros(local_attention_vec_size)) # v_l\n",
    "\n",
    "            #local_attn = Variable(torch.zeros(batch_size, local_attn_length))\n",
    "            local_attn = nn.Parameter(torch.FloatTensor(batch_size, local_attn_length))  \n",
    "            init.normal(local_v)\n",
    "            init.xavier_uniform(local_attn)  \n",
    "            \n",
    "            def local_attention(query):\n",
    "                # linear map\n",
    "                y = Linear(query, local_attention_vec_size, True)\n",
    "                y = y.view(-1, 1, 1, local_attention_vec_size)\n",
    "                # Attention mask is a softmax of v_l^{\\top} * tanh(...)\n",
    "                #print((local_v * torch.tanh(local_hidden_features + y)).size())\n",
    "                s = torch.sum(local_v * torch.tanh(local_hidden_features + y), dim=[1, 3])\n",
    "                # Now calculate the attention-weighted vector, i.e., alpha in eq.[2]\n",
    "                a = tf.softmax(s)\n",
    "                return a\n",
    "        \n",
    "        if global_flag:\n",
    "            global_attn_length = global_attention_states.data.size(1) # n_input_encoder\n",
    "            global_n_input = global_attention_states.data.size(2)\n",
    "            global_attn_size = global_attention_states.data.size(3) # n_steps_encoder\n",
    "\n",
    "            # A trick: to calculate U_l * x^{i,k} by a 1-by-1 convolution\n",
    "            global_hidden = global_attention_states.contiguous().view(-1, global_attn_size, global_attn_length, global_n_input)\n",
    "            # Size of query vectors for attention.\n",
    "            global_attention_vec_size = global_attn_size\n",
    "            global_k = nn.Conv2d(global_attn_size, global_attention_vec_size, (1,global_n_input), (1, 1))\n",
    "            global_hidden_features = global_k(global_hidden.float())\n",
    "\n",
    "            #global_v = Variable(torch.zeros(global_attention_vec_size)) # v_l\n",
    "            global_v = nn.Parameter(torch.FloatTensor(global_attention_vec_size)) \n",
    "\n",
    "            #global_attn = Variable(torch.zeros(batch_size, global_attn_length))\n",
    "            global_attn = nn.Parameter(torch.FloatTensor(batch_size, global_attn_length)) \n",
    "            init.normal(global_v)                        \n",
    "            init.xavier_uniform(global_attn)\n",
    "                                       \n",
    "            def global_attention(query):\n",
    "                # linear map\n",
    "                y = Linear(query, global_attention_vec_size, True)\n",
    "                y = y.view(-1, 1, 1, global_attention_vec_size)\n",
    "                # Attention mask is a softmax of v_g^{\\top} * tanh(...)\n",
    "                s = torch.sum(global_v * torch.tanh(global_hidden_features + y), dim=[1, 3])\n",
    "                a = tf.softmax(s)\n",
    "                \n",
    "                return a\n",
    "        \n",
    "        outputs = []\n",
    "        attn_weights = []\n",
    "        i = 0\n",
    "        # i is the index of the which time step\n",
    "        # local_inp is numpy.array and the shape of local_inp is (batch_size, n_feature)\n",
    "        for local_inp, global_inp in zip(local_inputs, global_inputs):\n",
    "            if local_flag and global_flag:\n",
    "                # multiply attention weights with the original input\n",
    "                #print(global_attn.size(), global_inp.size())\n",
    "                #print(local_attn.size(), local_inp.size())\n",
    "                local_x = local_attn * local_inp.float()\n",
    "                \n",
    "                global_x = global_attn * global_inp.float()\n",
    "                # Run the BasicLSTM with the newly input\n",
    "                xx = torch.cat([local_x, global_x], 1)\n",
    "                #print(xx.size())\n",
    "                cell_output, state = cell(xx)\n",
    "                # Run the attention mechanism.\n",
    "                #print(state.size())\n",
    "                local_attn = local_attention([state])\n",
    "                #print(local_attn.size())\n",
    "                global_attn = global_attention([state])\n",
    "                attn_weights.append((local_attn, global_attn))\n",
    "            elif local_flag:\n",
    "                local_x = local_attn * local_inp\n",
    "                cell_output, state = cell(local_x)\n",
    "                local_attn = local_attention([state])\n",
    "                attn_weights.append(local_attn)\n",
    "            elif global_flag:\n",
    "                global_x = global_attn * global_inp\n",
    "                cell_output, state = cell(global_x)\n",
    "                global_attn = global_attention([state])\n",
    "                attn_weights.append(global_attn)\n",
    "            # Attention output projection\n",
    "            output = cell_output\n",
    "            outputs.append(output)\n",
    "            i += 1\n",
    "            \n",
    "        return outputs, state, attn_weights\n",
    "\n",
    "    def temporal_attention(self, decoder_inputs, external_inputs, encoder_state, attention_states, \n",
    "                           cell, external_flag, output_size=64):\n",
    "            # Needed for reshaping.\n",
    "            batch_size = decoder_inputs[0].data.size(0)\n",
    "            attn_length = attention_states.data.size(1)\n",
    "            attn_size = attention_states.data.size(2)\n",
    "            \n",
    "            # A trick: to calculate W_d * h_o by a 1-by-1 convolution\n",
    "            # See at eq.[6] in the paper\n",
    "            hidden = attention_states.view(-1, attn_size, attn_length, 1) # need to reshape before\n",
    "            # Size of query vectors for attention.\n",
    "            attention_vec_size = attn_size\n",
    "            w_conv = nn.Conv2d(attn_size, attention_vec_size, (1,1), (1,1))\n",
    "            hidden_features = w_conv(hidden) \n",
    "            #v = Variable(torch.zeros(attention_vec_size)) # v_l\n",
    "            v = nn.Parameter(torch.FloatTensor(attention_vec_size)) \n",
    "            init.normal(v)       \n",
    "                             \n",
    "            def attention(query):\n",
    "                # linear map\n",
    "                y = Linear(query, attention_vec_size, True)\n",
    "                y = y.view(-1, 1, 1, attention_vec_size)\n",
    "                # Attention mask is a softmax of v_d^{\\top} * tanh(...).\n",
    "                s = torch.sum(v * torch.tanh(hidden_features + y), dim=[1, 3])\n",
    "                # Now calculate the attention-weighted vector, i.e., gamma in eq.[7]\n",
    "                a = tf.softmax(s)\n",
    "                # eq. [8]\n",
    "                #print(hidden.size())\n",
    "                #print((a.view(-1, 1, attn_length, 1)).size())\n",
    "                d = torch.sum(a.view(-1, 1, attn_length, 1)* hidden, dim=[2, 3])\n",
    "                    \n",
    "                return d.view(-1, attn_size)\n",
    "            \n",
    "            #attn = Variable(torch.zeros(batch_size, attn_size))\n",
    "            attn = nn.Parameter(torch.FloatTensor(batch_size, attn_size)) \n",
    "            init.xavier_uniform(attn)    \n",
    "                             \n",
    "            i = 0\n",
    "            outputs = []\n",
    "            prev = None\n",
    "            \n",
    "            for (inp, ext_inp) in zip(decoder_inputs, external_inputs):\n",
    "                # Merge input and previous attentions into one vector of the right size.\n",
    "                \n",
    "                input_size = inp.data.size(1)\n",
    "                #print(i, input_size)\n",
    "                #input_size是指向量维度\n",
    "                # we map the concatenation to shape [batch_size, input_size]\n",
    "                if external_flag:\n",
    "                    #print(inp.data.size(1),ext_inp.data.size(1),attn.data.size(1))\n",
    "                    x = Linear([inp.float()] + [ext_inp.float()] + [attn.float()], input_size, True)\n",
    "                else:\n",
    "                    x = Linear([inp.float()] + [attn.float()], input_size, True)\n",
    "                # Run the RNN.\n",
    "                #print(x.size())\n",
    "                cell_output, state = cell(x)\n",
    "                # Run the attention mechanism.\n",
    "                #print(state.size())\n",
    "                attn = attention([state])\n",
    "                \n",
    "                # Attention output projection\n",
    "                #print(cell_output.size(), attn.size())\n",
    "                output = Linear([cell_output] + [attn], output_size, True)\n",
    "                outputs.append(output)\n",
    "                i += 1\n",
    "            return outputs, state\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoder_attention_states, encoder_inputs, _labels, _external_inputs, decoder_inputs \\\n",
    "            = input_transform(x)\n",
    "            \n",
    "        encoder_outputs, encoder_state, attn_weights = self.spatial_attention(encoder_inputs,\n",
    "                                                                              encoder_attention_states,\n",
    "                                                                              self.encoder_cell,\n",
    "                                                                              self.hps['s_attn_flag'])\n",
    "\n",
    "        # Calculate a concatenation of encoder outputs to put attention on.\n",
    "        top_states = [e.view(-1, 1, 64) for e in encoder_outputs]\n",
    "        attention_states = torch.cat(top_states, 1)\n",
    "\n",
    "        # the implement of decoder\n",
    "        decoder_outputs, states = self.temporal_attention(decoder_inputs,\n",
    "                                                          _external_inputs,\n",
    "                                                          encoder_state,\n",
    "                                                          attention_states,\n",
    "                                                          self.decoder_cell,\n",
    "                                                          self.hps['ext_flag'])\n",
    "        \n",
    "        preds = [torch.matmul(i, self.w_out) + self.b_out for i in decoder_outputs]\n",
    "        \n",
    "        return preds, _labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tensorflow04/anaconda3/envs/xuchao/lib/python3.5/site-packages/ipykernel_launcher.py:57: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/home/tensorflow04/anaconda3/envs/xuchao/lib/python3.5/site-packages/ipykernel_launcher.py:58: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "/home/tensorflow04/anaconda3/envs/xuchao/lib/python3.5/site-packages/ipykernel_launcher.py:96: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "/home/tensorflow04/anaconda3/envs/xuchao/lib/python3.5/site-packages/ipykernel_launcher.py:97: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/home/tensorflow04/anaconda3/envs/xuchao/lib/python3.5/site-packages/ipykernel_launcher.py:127: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "/home/tensorflow04/anaconda3/envs/xuchao/lib/python3.5/site-packages/ipykernel_launcher.py:128: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/home/xuchao/th_geoman/utils.py:28: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(weights)\n",
      "/home/tensorflow04/anaconda3/envs/xuchao/lib/python3.5/site-packages/ipykernel_launcher.py:107: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/tensorflow04/anaconda3/envs/xuchao/lib/python3.5/site-packages/ipykernel_launcher.py:136: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/tensorflow04/anaconda3/envs/xuchao/lib/python3.5/site-packages/ipykernel_launcher.py:196: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "/home/tensorflow04/anaconda3/envs/xuchao/lib/python3.5/site-packages/ipykernel_launcher.py:215: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/home/tensorflow04/anaconda3/envs/xuchao/lib/python3.5/site-packages/ipykernel_launcher.py:205: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------epoch 0-----------\n",
      "17.509355545043945\n",
      "[106 107 108 109 110 111 112 113 114 115]\n",
      "(10, 12, 35)\n",
      "===============METRIC===============\n",
      "rmse = 0.125884\n",
      "mae = 0.975090\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = GeoMAN(hps)\n",
    "\n",
    "total_epoch =1\n",
    "batch_size = 16\n",
    "lr = 0.00001\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=lr, momentum=0.9)\n",
    "def criterion(preds, labels):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss = 0.0\n",
    "    \n",
    "    for ps, ls in zip(preds, labels):\n",
    "        loss += loss_fn(ps.float(),ls.float())\n",
    "        \n",
    "    return loss\n",
    "\n",
    "for i in range(total_epoch):\n",
    "    print('----------epoch {}-----------'.format(i))\n",
    "    training_data = shuffle_data(training_data)\n",
    "    lossSum = 0\n",
    "    i += 1\n",
    "    for j in range(0, num_train, batch_size):\n",
    "        x = get_batch_feed_dict(j, batch_size, training_data, global_inpts, global_attn_sts)\n",
    "        preds, labels = model(x)\n",
    "        loss = criterion(preds, labels)\n",
    "        lossSum += loss.data.numpy()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(lossSum)\n",
    "#print(preds, labels)\n",
    "\n",
    "# test\n",
    "n_split_test = 2\n",
    "test_loss = 0\n",
    "test_indexes = np.int64(\n",
    "    np.linspace(0, num_valid, n_split_test))\n",
    "rmses=[]\n",
    "maes=[]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "for k in range(n_split_test - 1):\n",
    "    x = get_valid_batch_feed_dict(k, test_indexes, valid_data, global_inpts, global_attn_sts)\n",
    "    # re-scale predicted labels\n",
    "    batch_preds, _ = model(x)\n",
    "    batch_preds = np.array([bp.data.numpy() for bp in  batch_preds ])\n",
    "    batch_preds = np.swapaxes(batch_preds, 0, 1)\n",
    "    batch_preds = np.reshape(batch_preds, [batch_preds.shape[0], -1])\n",
    "    # re-scale real labels\n",
    "    batch_labels = test_data[4]\n",
    "    batch_labels = batch_labels[test_indexes[k]:test_indexes[k + 1]]\n",
    "    rmses.append(np.sqrt(np.sum(np.square(batch_labels-batch_preds))/\n",
    "                         (batch_labels.shape[0]*batch_labels.shape[1])))\n",
    "    maes.append((np.abs(batch_labels-batch_preds)).mean())\n",
    "\n",
    "test_rmses = np.asarray(rmses)\n",
    "test_maes = np.asarray(maes)\n",
    "\n",
    "print('===============METRIC===============')\n",
    "print('rmse = {:.6f}'.format(test_rmses.mean()))\n",
    "print('mae = {:.6f}'.format(test_maes.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
